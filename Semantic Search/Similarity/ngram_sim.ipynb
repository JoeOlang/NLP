{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>2215</td>\n",
       "      <td>Let M be an almost complex manifold equipped w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>10791</td>\n",
       "      <td>&lt;PLOT &gt; is a collection of routines to draw su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22072</th>\n",
       "      <td>22072</td>\n",
       "      <td>The article analyzes a proposed network topolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62703</th>\n",
       "      <td>62706</td>\n",
       "      <td>The ICML 2013 Workshop on Challenges in Repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35145</th>\n",
       "      <td>35145</td>\n",
       "      <td>The Internet has revolutionized the computer a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           abstract\n",
       "2215    2215  Let M be an almost complex manifold equipped w...\n",
       "10791  10791  <PLOT > is a collection of routines to draw su...\n",
       "22072  22072  The article analyzes a proposed network topolo...\n",
       "62703  62706  The ICML 2013 Workshop on Challenges in Repres...\n",
       "35145  35145  The Internet has revolutionized the computer a..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('sampledata/data_input.csv')     # Read apstracts from csv file\n",
    "data.sample(5)                                     # Show 5 rows of data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('id', axis = 1, inplace = True)            # Drop id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('sampledata/data.csv', index = False)  # Save data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "vectorizer = CountVectorizer()                     # Create a vectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['abstract'].tolist()                  # Convert abstracts to list\n",
    "texts = [str(text) for text in texts]              # Convert all elements to string\n",
    "texts = [text.lower() for text in texts]           # Convert all elements to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86209\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))                                  # Print number of abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stexts = texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(stexts)        # Create a matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(features))                              # Print type of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)                      # Print vocabulary, the underscore is a placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.vocabulary_))                 # Print number of words in vocabulary i.e. vocabulary size. The size generally depends on the size of the corpus and thus affects the number of features (& performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = euclidean_distances(features)          # Calculate euclidean distances between all pairs of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# save distance to file\n",
    "import numpy as np\n",
    "np.savetxt('sampledata/distances.csv', distances, delimiter = ',')\n",
    "\n",
    "# load distance from file\n",
    "distances = np.loadtxt('sampledata/distances.csv', delimiter = ',')\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x692 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 944 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.60681686 23.4520788  17.2626765  15.5241747  11.35781669 24.71841419\n",
      "  14.69693846 10.04987562 18.84144368 19.6977156 ]]\n"
     ]
    }
   ],
   "source": [
    "new_query = 'The purpose of this study is to investigate the effect of the COVID-19 pandemic on the mental health of the population in the United States.'\n",
    "\n",
    "new_query = [new_query]                            # Convert query to list\n",
    "\n",
    "new_query = [str(text) for text in new_query]      # Convert all elements to string\n",
    "new_query = [text.lower() for text in new_query]   # Convert all elements to lowercase\n",
    "\n",
    "new_query_features = vectorizer.transform(new_query) # Create a matrix of features for the query\n",
    "\n",
    "new_query_distances = euclidean_distances(new_query_features, features) # Calculate euclidean distances between the query and all abstracts\n",
    "\n",
    "print()\n",
    "print(new_query_distances)                         # Print distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(sample_text):\n",
    "    sample_text = [sample_text]                                 # Convert query to list\n",
    "    sample_text = [str(text) for text in sample_text]           # Convert all elements to string\n",
    "    sample_text = [text.lower() for text in sample_text]        # Convert all elements to lowercase\n",
    "    sample_text_features = vectorizer.transform(sample_text)    # Create a matrix of features for the query\n",
    "    return sample_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(sample_text_features, features):\n",
    "    i = 0\n",
    "    \n",
    "    for item in features:\n",
    "        print(stexts[i])\n",
    "        i += 1\n",
    "        print(euclidean_distances(sample_text_features, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turing machines and g\\\"odel numbers are important pillars of the theory of computation. thus, any computational architecture needs to show how it could relate to turing machines and how stable implementations of turing computation are possible. in this chapter, we implement universal turing computation in a neural field environment. to this end, we employ the canonical symbologram representation of a turing machine obtained from a g\\\"odel encoding of its symbolic repertoire and generalized shifts. the resulting nonlinear dynamical automaton (nda) is a piecewise affine-linear map acting on the unit square that is partitioned into rectangular domains. instead of looking at point dynamics in phase space, we then consider functional dynamics of probability distributions functions (p.d.f.s) over phase space. this is generally described by a frobenius-perron integral transformation that can be regarded as a neural field equation over the unit square as feature space of a dynamic field theory (dft). solving the frobenius-perron equation yields that uniform p.d.f.s with rectangular support are mapped onto uniform p.d.f.s with rectangular support, again. we call the resulting representation \\emph{dynamic field automaton}.\n",
      "[[17.60681686]]\n",
      "rna-sequencing has revolutionized biomedical research and, in particular, our ability to study gene alternative splicing. the problem has important implications for human health, as alternative splicing may be involved in malfunctions at the cellular level and multiple diseases. however, the high-dimensional nature of the data and the existence of experimental biases pose serious data analysis challenges. we find that the standard data summaries used to study alternative splicing are severely limited, as they ignore a substantial amount of valuable information. current data analysis methods are based on such summaries and are hence suboptimal. further, they have limited flexibility in accounting for technical biases. we propose novel data summaries and a bayesian modeling framework that overcome these limitations and determine biases in a nonparametric, highly flexible manner. these summaries adapt naturally to the rapid improvements in sequencing technology. we provide efficient point estimates and uncertainty assessments. the approach allows to study alternative splicing patterns for individual samples and can also be the basis for downstream analyses. we found a severalfold improvement in estimation mean square error compared popular approaches in simulations, and substantially higher consistency between replicates in experimental data. our findings indicate the need for adjusting the routine summarization and analysis of alternative splicing rna-seq studies. we provide a software implementation in the r package casper.\n",
      "[[23.4520788]]\n",
      "queuing models provide insight into the temporal inhomogeneity of human dynamics, characterized by the broad distribution of waiting times of individuals performing tasks. we study the queuing model of an agent trying to execute a task of interest, the priority of which may vary with time due to the agent's \"state of mind.\" however, its execution is disrupted by other tasks of random priorities. by considering the priority of the task of interest either decreasing or increasing algebraically in time, we analytically obtain and numerically confirm the bimodal and unimodal waiting time distributions with power-law decaying tails, respectively. these results are also compared to the updating time distribution of papers in the arxiv.org and the processing time distribution of papers in physical review journals. our analysis helps to understand human task execution in a more realistic scenario.\n",
      "[[17.2626765]]\n",
      "in a multiple-object auction, every bidder tries to win as many objects as possible with a bidding algorithm. this paper studies position-randomized auctions, which form a special class of multiple-object auctions where a bidding algorithm consists of an initial bid sequence and an algorithm for randomly permuting the sequence. we are especially concerned with situations where some bidders know the bidding algorithms of others. for the case of only two bidders, we give an optimal bidding algorithm for the disadvantaged bidder. our result generalizes previous work by allowing the bidders to have unequal budgets. one might naturally anticipate that the optimal expected numbers of objects won by the bidders would be proportional to their budgets. surprisingly, this is not true. our new algorithm runs in optimal o(n) time in a straightforward manner. the case with more than two bidders is open.\n",
      "[[15.5241747]]\n",
      "in arxiv:1109.6438v1 [math.ag] we introduced and studied a notion of algebraic entropy. in this paper we will give an application of algebraic entropy in proving kunz regularity criterion for all contracting self-maps of finite length of noetherian local rings in arbitrary characteristic. some conditions of kunz criterion have already been extended to the general case by avramov, iyengar and miller in arxiv:math/0312412v2 [math.ac], using different methods.\n",
      "[[11.35781669]]\n",
      "we consider a set of k autonomous robots that are endowed with visibility sensors (but that are otherwise unable to communicate) and motion actuators. those robots must collaborate to reach a sin- gle vertex that is unknown beforehand, and to remain there hereafter. previous works on gathering in ring-shaped networks suggest that there exists a tradeoff between the size of the set of potential initial configurations, and the power of the sensing capabilities of the robots (i.e. the larger the initial configuration set, the most powerful the sensor needs to be). we prove that there is no such trade off. we propose a gathering protocol for an odd number of robots in a ring-shaped network that allows symmetric but not periodic configurations as initial configurations, yet uses only local weak multiplicity detection. robots are assumed to be anonymous and oblivious, and the execution model is the non-atomic corda model with asynchronous fair scheduling. our protocol allows the largest set of initial configurations (with respect to impossibility results) yet uses the weakest multiplicity detector to date. the time complexity of our protocol is o(n2), where n denotes the size of the ring. compared to previous work that also uses local weak multiplicity detection, we do not have the constraint that k < n/2 (here, we simply have 2 < k < n - 3).\n",
      "[[24.71841419]]\n",
      "the fermilab linac delivers a variable intensity, 400-mev beam to the the mucool test area experimental hall via a beam line specifically designed to facilitate measurements of the linac beam emittance and properties. a 10 m, dispersion-free and magnet-free straight utilizes an upstream quadrupole focusing triplet in combination with the necessary in-straight beam diagnostics to fully characterize the transverse beam properties. since the linac does not produce a strictly elliptical phase space, tomography must be performed on the profile data to retrieve the actual particle distribution in phase space. this is achieved by rotating the phase space distribution using different waist focusing conditions of the upstream triplet and performing a de-convolution of the profile data. preliminary measurements using this diagnostic section are reported here.\n",
      "[[14.69693846]]\n",
      "let r be the quotient of a polynomial ring over a field k by an ideal generated by monomials. we derive a formula for the multigraded poincare' series of r, i.e., the generating function for the ranks of the modules in a minimal multigraded free resolution of k over r. the formula can be expressed in terms of the homology of lower intervals in a certain lattice associated to the minimal set of generators for the ideal.\n",
      "[[10.04987562]]\n",
      "discontinuity preserving smoothing is a fundamentally important procedure that is useful in a wide variety of image processing contexts. it is directly useful for noise reduction, and frequently used as an intermediate step in higher level algorithms. for example, it can be particularly useful in edge detection and segmentation. three well known algorithms for discontinuity preserving smoothing are nonlinear anisotropic diffusion, bilateral filtering, and mean shift filtering. although slight differences make them each better suited to different tasks, all are designed to preserve discontinuities while smoothing. however, none of them satisfy this goal perfectly: they each have exception cases in which smoothing may occur across hard edges. the principal contribution of this paper is the identification of a property we call edge awareness that should be satisfied by any discontinuity preserving smoothing algorithm. this constraint can be incorporated into existing algorithms to improve quality, and usually has negligible changes in runtime performance and/or complexity. we present modifications necessary to augment diffusion and mean shift, as well as a new formulation of the bilateral filter that unifies the spatial and range spaces to achieve edge awareness.\n",
      "[[18.84144368]]\n",
      "we investigate the properties of the hybrid monte-carlo algorithm (hmc) in high dimensions. hmc develops a markov chain reversible w.r.t. a given target distribution $\\pi$ by using separable hamiltonian dynamics with potential $-\\log\\pi$. the additional momentum variables are chosen at random from the boltzmann distribution and the continuous-time hamiltonian dynamics are then discretised using the leapfrog scheme. the induced bias is removed via a metropolis-hastings accept/reject rule. in the simplified scenario of independent, identically distributed components, we prove that, to obtain an $\\mathcal{o}(1)$ acceptance probability as the dimension $d$ of the state space tends to $\\infty$, the leapfrog step-size $h$ should be scaled as $h= l \\times d^{-1/4}$. therefore, in high dimensions, hmc requires $\\mathcal{o}(d^{1/4})$ steps to traverse the state space. we also identify analytically the asymptotically optimal acceptance probability, which turns out to be 0.651 (to three decimal places). this is the choice which optimally balances the cost of generating a proposal, which {\\em decreases} as $l$ increases, against the cost related to the average number of proposals required to obtain acceptance, which {\\em increases} as $l$ increases.\n",
      "[[19.6977156]]\n"
     ]
    }
   ],
   "source": [
    "get_distances(get_feature(new_query), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for item in features:\n",
    "    print(stexts[i])\n",
    "    i += 1\n",
    "    print(euclidean_distances(features[0], item)[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4524e0aa81b3ae16e25dcf9b33e91112c133dfc9ef43cc517d5da5b8fe0f3eee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
